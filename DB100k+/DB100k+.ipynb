{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8283c7d1-058d-4dc2-831a-1a293825f3b0",
   "metadata": {},
   "source": [
    "# DB100k dataset semantically enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb4ba75-ebcb-4e1c-b4f0-9b8737909b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pickle import dump, load, HIGHEST_PROTOCOL\n",
    "from os import makedirs\n",
    "from os.path import exists\n",
    "from urllib.request import urlopen\n",
    "from bz2 import open as bz2open\n",
    "from shutil import copy as copy_file\n",
    "\n",
    "try:\n",
    "    from rdflib import Graph\n",
    "except:\n",
    "    !pip install rdflib\n",
    "    !conda install -c plotly plotly-orca -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5cd78bd-856f-42d9-9457-45dcd13cb1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_folder=f\"../draft/DB100k-DBP\"\n",
    "destination_folder=f\".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae926bdb-736d-4f23-8172-7e500346edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_folder=f\"{draft_folder}{'' if draft_folder.endswith('/') else '/'}\"\n",
    "destination_folder=f\"{destination_folder}{'' if destination_folder.endswith('/') else '/'}\"\n",
    "\n",
    "DB93k_folder_url = \"https://github.com/nicolas-hbt/benchmark-sematk/raw/refs/heads/main/datasets/DB93K/\"\n",
    "DB100k_folder_url = \"https://github.com/iieir-km/ComplEx-NNE_AER/raw/refs/heads/master/datasets/DB100K/\"\n",
    "DBPedia_archive_url = \"https://downloads.dbpedia.org/2016-10/core/\"\n",
    "DBPedia_ontology_url = \"http://downloads.dbpedia.org/2016-10/dbpedia_2016-10.owl\"\n",
    "\n",
    "DB93k_draft_folder=f\"{draft_folder}DB93k/\"\n",
    "DB100k_draft_folder=f\"{draft_folder}DB100k/\"\n",
    "DBpedia_draft_folder=f\"{draft_folder}DBpedia/\"\n",
    "\n",
    "dest_txt_folder=destination_folder\n",
    "dest_pkl_folder=f\"{destination_folder}pickle/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70930493-f791-48a4-ae58-227182144d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in [\n",
    "    DB93k_draft_folder,\n",
    "    DB100k_draft_folder,\n",
    "    DBpedia_draft_folder,\n",
    "    dest_txt_folder,\n",
    "    dest_pkl_folder\n",
    "]: \n",
    "    makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71238dfb-c2f0-4288-bd4b-b223cc1c8c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm([\n",
    "    \"rel2id.pkl\",\n",
    "    \"class2id.pkl\",\n",
    "    \"ent2id.pkl\",\n",
    "    \"class2id.pkl\",\n",
    "    \"rel2dom.pkl\",\n",
    "    \"rel2range.pkl\",\n",
    "    \"subclassof2id.pkl\"\n",
    "]):\n",
    "    file_uri=f\"{DB93k_folder_url}{file}\"\n",
    "    file_path = f\"{DB93k_draft_folder}{file}\"\n",
    "    \n",
    "    if exists(file_path):\n",
    "        continue\n",
    "        \n",
    "    with urlopen(file_uri) as file_online:\n",
    "        with open(file_path, \"wb\") as file_local:\n",
    "            file_local.write(file_online.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "386861ad-b33d-47d1-8126-df74cfffd603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm([\n",
    "    \"_train.txt\",\n",
    "    \"_test.txt\",\n",
    "    \"_valid.txt\",\n",
    "]):\n",
    "    file_uri=f\"{DB100k_folder_url}{file}\"\n",
    "    file_path = f\"{DB100k_draft_folder}{file}\"\n",
    "    \n",
    "    if exists(file_path):\n",
    "        continue\n",
    "        \n",
    "    with urlopen(file_uri) as file_online:\n",
    "        with open(file_path, \"wb\") as file_local:\n",
    "            file_local.write(file_online.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e02b8d73-242b-45a0-8eae-b6762e343e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(f\"{DBpedia_draft_folder}dbpedia_2016-10.owl\"):\n",
    "    with urlopen(DBPedia_ontology_url) as file_online:\n",
    "        with open(f\"{DBpedia_draft_folder}dbpedia_2016-10.owl\", \"wb\") as file_local:\n",
    "            file_local.write(file_online.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba38f522-d4c2-42fb-a305-e99645852303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading interlanguage_links_chapters_en.ttl:   0%|                                           | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(file_path):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urlopen(file_uri) \u001b[38;5;28;01mas\u001b[39;00m file_online:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_local:\n\u001b[0;32m     16\u001b[0m         file_local\u001b[38;5;241m.\u001b[39mwrite(file_online\u001b[38;5;241m.\u001b[39mread())\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\urllib\\request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\urllib\\request.py:521\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    520\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 521\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\urllib\\request.py:630\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 630\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\urllib\\request.py:559\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    558\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\urllib\\request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\urllib\\request.py:639\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "with tqdm([\n",
    "    \"interlanguage_links_chapters_en.ttl\",\n",
    "    \"instance_types_en.ttl\",\n",
    "]) as bar:\n",
    "    for file in bar:\n",
    "        bar.set_description(f\"Downloading {file}\")\n",
    "\n",
    "        file_uri=f\"{DBPedia_archive_url}{file}.bz2\"\n",
    "        file_path=f\"{DBpedia_draft_folder}{file}.bz2\"\n",
    "\n",
    "        if exists(file_path):\n",
    "            continue\n",
    "            \n",
    "        with urlopen(file_uri) as file_online:\n",
    "            with open(file_path, \"wb\") as file_local:\n",
    "                file_local.write(file_online.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510cc99c-870c-44d1-a01c-edd665fb199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm([\n",
    "    \"interlanguage_links_chapters_en.ttl\",\n",
    "    \"instance_types_en.ttl\",\n",
    "]) as bz2_bar:\n",
    "    for file in bz2_bar:\n",
    "        bz2_bar.set_description(f\"Decompressing {file}\")\n",
    "\n",
    "        ttl_file_path=f\"{DBpedia_draft_folder}{file}\"\n",
    "        bz2_file_path=f\"{ttl_file_path}.bz2\"\n",
    "\n",
    "        if exists(ttl_file_path):\n",
    "            continue\n",
    "        \n",
    "        with bz2open(bz2_file_path, \"r\") as bz2_file:\n",
    "            with open(ttl_file_path, \"w\", encoding=\"utf-8\") as ttl_file:\n",
    "                for line in bz2_file:\n",
    "                    ttl_file.write(line.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75470622-e872-4451-b7e0-3cbf74e42295",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace={\n",
    "    \"www.monolithgraphics.com\": \"Q1969125\",\n",
    "    \"?autoplay=true\": \"Q477993\",\n",
    "    \"player.html\": \"Q7952744\"\n",
    "}\n",
    "replace={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3db6d17-f718-420e-a7e9-246dba4e456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_100=set([])\n",
    "predicates_100=set([])\n",
    "count_100=0\n",
    "train_100=0\n",
    "test_100=0\n",
    "valid_100=0\n",
    "\n",
    "for split in [\"train\", \"test\", \"valid\"]:\n",
    "    file_path=f\"{DB100k_draft_folder}_{split}.txt\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            s, p, o = line.strip().split(\"\\t\")\n",
    "            s = replace[s] if s in replace.keys() else s\n",
    "            o = replace[o] if o in replace.keys() else o\n",
    "            predicates_100.add(p)\n",
    "            entities_100.add(s)\n",
    "            entities_100.add(o)\n",
    "            count_100+=1\n",
    "            if split==\"train\":\n",
    "                train_100+=1\n",
    "            if split==\"test\":\n",
    "                test_100+=1\n",
    "            if split==\"valid\":\n",
    "                valid_100+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62142f47-9314-49e0-bfda-2339b73c7954",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(predicates_100)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a56b9ad-7ce3-4ac6-8e8c-d8dd6d252348",
   "metadata": {},
   "source": [
    "# ent2id, rel2id, class2id\n",
    "## rel2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44c7f6a-8ac9-4df0-9671-216bf65fc662",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicates_100=[\n",
    "    f\"<http://dbpedia.org/ontology/{predicate}>\"\n",
    "    for predicate in predicates_100\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afddfce0-eed2-4749-855c-22be7cec7aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2id_100={p: i for i, p in enumerate(predicates_100)}\n",
    "id2rel_100={i: p for p, i in rel2id_100.items()}\n",
    "inverse_predicate_offset = max(rel2id_100.values()) + 1\n",
    "\n",
    "with open(f\"{dest_pkl_folder}rel2id.pkl\", \"wb\") as handle:\n",
    "    dump(rel2id_100, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd31052-d5ae-42b1-8d45-ba7cef42e804",
   "metadata": {},
   "source": [
    "## ent2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4490c9-da58-467e-8ae2-8f716d4c20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_dbpedia={}\n",
    "nb_lines=sum(1 for _ in open(f\"{DBpedia_draft_folder}interlanguage_links_chapters_en.ttl\", \"r\", encoding=\"utf-8\"))\n",
    "with open(f\"{DBpedia_draft_folder}interlanguage_links_chapters_en.ttl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in tqdm(enumerate(f), total=nb_lines):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        s, p, o = line.strip().split(\" \")[:3]\n",
    "        if not s.startswith(\"<http://dbpedia.org/resource/\"):\n",
    "            continue\n",
    "        if not o.startswith(\"<http://www.wikidata.org/entity/\"):\n",
    "            continue\n",
    "        o = o[len(\"<http://www.wikidata.org/entity/\"):-1]\n",
    "        if not o in entities_100:\n",
    "            continue\n",
    "        wikidata_dbpedia[o]=s\n",
    "\n",
    "unknown_entities_100=[\n",
    "    x for x in entities_100\n",
    "    if not x in wikidata_dbpedia\n",
    "]\n",
    "\n",
    "for k in unknown_entities_100:\n",
    "    wikidata_dbpedia[k]=k\n",
    "\n",
    "with open(f\"{dest_pkl_folder}wikidata_dbpedia.pkl\", \"wb\") as handle:\n",
    "    dump(wikidata_dbpedia, handle)\n",
    "\n",
    "ent2id_100 = {wikidata_dbpedia[k]: i for i, k in enumerate(wikidata_dbpedia.keys())}\n",
    "id2ent_100 = {v: k for k, v in ent2id_100.items()}\n",
    "\n",
    "with open(f\"{dest_pkl_folder}ent2id.pkl\", \"wb\") as handle:\n",
    "    dump(wikidata_dbpedia, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e8f839-5395-48c7-b9b0-94769bb6cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_entities_100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e947a77c-9e3e-4288-871b-53081b9599dc",
   "metadata": {},
   "source": [
    "### Encode train, test, valid splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b01e30-cfbe-45aa-9113-e5c123367987",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\", \"test\", \"valid\"]:\n",
    "    nb_lines=sum(1 for _ in open(f\"{DB100k_draft_folder}_{split}.txt\", \"r\", encoding=\"utf-8\"))\n",
    "    with open(f\"{DB100k_draft_folder}_{split}.txt\", \"r\", encoding='utf-8') as r:\n",
    "        with open(f\"{dest_txt_folder}{split}2id.txt\", \"w+\", encoding='utf-8') as w:\n",
    "            with tqdm(enumerate(r), total=nb_lines) as bar:\n",
    "                bar.set_description(f\"Encoding {split}2id.txt\")\n",
    "                for i, line in bar:\n",
    "                    s, p, o = line.strip().split(\"\\t\")\n",
    "                    s = replace[s] if s in replace.keys() else s\n",
    "                    o = replace[o] if o in replace.keys() else o\n",
    "                    encoded_s = ent2id_100[wikidata_dbpedia[s]]\n",
    "                    encoded_p = rel2id_100[f\"<http://dbpedia.org/ontology/{p}>\"]\n",
    "                    encoded_o = ent2id_100[wikidata_dbpedia[o]]\n",
    "                    w.write(f\"{encoded_s}\\t{encoded_p}\\t{encoded_o}\\n\")   \n",
    "        copy_file(f\"{dest_txt_folder}{split}2id.txt\", f\"{dest_txt_folder}{split}2id_inv.txt\")\n",
    "        \n",
    "        with open(f\"{dest_txt_folder}{split}2id.txt\", \"r\", encoding='utf-8') as r:\n",
    "            with open(f\"{dest_txt_folder}{split}2id_inv.txt\", \"a\", encoding='utf-8') as a:\n",
    "                with tqdm(enumerate(r), total=nb_lines) as bar:\n",
    "                    bar.set_description(f\"Encoding {split}2id_inv.txt\")\n",
    "                    for i, line in bar:\n",
    "                        s, p, o = line.strip().split(\"\\t\")\n",
    "                        p = int(p)\n",
    "                        a.write(f\"{o}\\t{p+inverse_predicate_offset}\\t{s}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0612d1fb-ba6c-4e93-8e15-bd6c8c41de2e",
   "metadata": {},
   "source": [
    "# observed_heads_original_kg, observed_tails_original_kg, observed_heads_inv, observed_tails_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2935c7bc-d203-45bd-8c39-c999907c7fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_heads_original_kg={}\n",
    "observed_heads_inv={}\n",
    "observed_tails_original_kg={}\n",
    "observed_tails_inv={}\n",
    "\n",
    "def observe(d, a, b, c):\n",
    "    if not a in d.keys():\n",
    "        d[a]={}\n",
    "\n",
    "    if not b in d[a].keys():\n",
    "        d[a][b]=[c]\n",
    "    else:\n",
    "        d[a][b].append(c)\n",
    "\n",
    "\n",
    "for split in [\"train\", \"test\", \"valid\"]:\n",
    "    nb_lines=sum(1 for _ in open(f\"{dest_txt_folder}{split}2id.txt\", \"r\", encoding=\"utf-8\"))\n",
    "    with open(f\"{dest_txt_folder}{split}2id.txt\", \"r\", encoding='utf-8') as r:\n",
    "        with tqdm(enumerate(r), total=nb_lines) as bar:\n",
    "            bar.set_description(f\"Indexing triples from {split}2id.txt\")\n",
    "            for i, line in bar:\n",
    "                s, p, o = line.strip().split(\"\\t\")\n",
    "                s, p, o = int(s), int(p), int(o)\n",
    "\n",
    "                observe(observed_tails_original_kg, s, p, o)\n",
    "                observe(observed_tails_inv, s, p, o)\n",
    "                observe(observed_tails_inv, o, p + inverse_predicate_offset, s)\n",
    "\n",
    "                observe(observed_heads_original_kg, o, p, s)\n",
    "                observe(observed_heads_inv, o, p, s)\n",
    "                observe(observed_heads_inv, s, p + inverse_predicate_offset, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf19a8-18b1-49df-91cc-d07ef1a7aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{dest_pkl_folder}observed_heads_original_kg.pkl\", \"wb\") as handle:\n",
    "    dump(observed_heads_original_kg, handle)\n",
    "\n",
    "with open(f\"{dest_pkl_folder}observed_heads_inv.pkl\", \"wb\") as handle:\n",
    "    dump(observed_heads_inv, handle)\n",
    "\n",
    "with open(f\"{dest_pkl_folder}observed_tails_original_kg.pkl\", \"wb\") as handle:\n",
    "    dump(observed_tails_original_kg, handle)\n",
    "\n",
    "with open(f\"{dest_pkl_folder}observed_tails_inv.pkl\", \"wb\") as handle:\n",
    "    dump(observed_tails_inv, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5f15ca-1bcb-4524-a09a-a8372dcc919b",
   "metadata": {},
   "source": [
    "## Class2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342aea0-56ef-44d2-bfda-1a1c51f8ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_types_100={}\n",
    "classes_100=set([])\n",
    "\n",
    "nb_lines=sum(1 for _ in open(f\"{DBpedia_draft_folder}instance_types_en.ttl\", \"r\", encoding=\"utf-8\"))\n",
    "with open(f\"{DBpedia_draft_folder}instance_types_en.ttl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in tqdm(enumerate(f), total=nb_lines):\n",
    "        if i ==0:\n",
    "            continue\n",
    "        s, p, o = line.strip().split(\" \")[:3]\n",
    "\n",
    "        if not s in ent2id_100:\n",
    "            continue\n",
    "\n",
    "        if not o.startswith(\"<http://dbpedia.org/ontology/\"):\n",
    "            continue\n",
    "        \n",
    "        classes_100.add(o)\n",
    "        instance_types_100[ent2id_100[s]]=o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dce4df-6ffd-49b0-822a-9fd7b811c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_graph = Graph()\n",
    "ontology_graph.parse(f\"{DBpedia_draft_folder}dbpedia_2016-10.owl\", format=\"xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e879105-4212-45a9-a574-51bff550525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicates_values = \"\\n\".join([\n",
    "    f\"({p})\"\n",
    "    for p in predicates_100\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8425bc20-b6e0-4d80-85da-e679111747f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "signatures = [\n",
    "    (f\"<{str(predicate)}>\", str(sign).split(\"#\")[-1], f\"<{str(classed)}>\")\n",
    "    for (predicate, sign, classed) in ontology_graph.query(\"\"\"\n",
    "    SELECT ?p ?sign ?x WHERE {\n",
    "        VALUES (?p) {\n",
    "            $PREDICATE_VALUES\n",
    "        }\n",
    "        VALUES (?sign) {\n",
    "            (<http://www.w3.org/2000/01/rdf-schema#domain>)\n",
    "            (<http://www.w3.org/2000/01/rdf-schema#range>)\n",
    "        }\n",
    "        ?p ?sign ?x\n",
    "    }\n",
    "    \"\"\".replace(\"$PREDICATE_VALUES\", predicates_values))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76feed0f-6d5a-4b1a-bcce-e4c6861ac7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2domain={\n",
    "    predicate: domain\n",
    "    for predicate, signature, domain in signatures\n",
    "    if signature == \"domain\"\n",
    "    and domain.startswith(\"<http://dbpedia.org/ontology/\")\n",
    "}\n",
    "\n",
    "rel2range={\n",
    "    predicate: range\n",
    "    for predicate, signature, range in signatures\n",
    "    if signature == \"range\"\n",
    "    and range.startswith(\"<http://dbpedia.org/ontology/\")\n",
    "}\n",
    "\n",
    "for c in rel2domain.values():\n",
    "    classes_100.add(c)\n",
    "\n",
    "\n",
    "for c in rel2range.values():\n",
    "    classes_100.add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a2c649-46a0-4fa3-8626-2124ce8efec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in list(rel2domain.items())[:5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b25c8-7e53-4366-9afa-88ece56e927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in list(rel2range.items())[:5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77876063-adfb-4f83-969e-c86c47ba5b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classes_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f0063c-e727-4ea2-8a6e-10d17d830c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_classes = {}\n",
    "\n",
    "discovered_classes = list(classes_100)\n",
    "\n",
    "while len(discovered_classes) > 0:\n",
    "    iteration_request=\"\"\"\n",
    "                SELECT ?onto_class ?super_class WHERE {\n",
    "                    VALUES (?onto_class) {\n",
    "                        $CLASSES_VALUES\n",
    "                    }\n",
    "                    ?onto_class <http://www.w3.org/2000/01/rdf-schema#subClassOf> ?super_class .\n",
    "                    filter(strstarts(str(?super_class), \"http://dbpedia.org/ontology/\"))\n",
    "                }\n",
    "            \"\"\".replace(\n",
    "            \"$CLASSES_VALUES\",\n",
    "            \"\\n\".join([\n",
    "                    f\"{' ' * 4 * 5}({c})\"\n",
    "                    for c in discovered_classes\n",
    "                ]\n",
    "             )\n",
    "        )\n",
    "\n",
    "    iteration_superclasses = [\n",
    "        (f\"<{str(onto_class)}>\", f\"<{str(super_class)}>\")\n",
    "        for (onto_class, super_class) in ontology_graph.query(iteration_request)\n",
    "    ]\n",
    "    \n",
    "    for onto_class, super_class in iteration_superclasses:\n",
    "        super_classes[onto_class] = super_class\n",
    "\n",
    "    discovered_classes = list(set([\n",
    "        super_class\n",
    "        for _, super_class in iteration_superclasses\n",
    "        if not super_class in super_classes.keys()\n",
    "    ]))\n",
    "\n",
    "for onto_class, super_class in super_classes.items():\n",
    "    classes_100.add(onto_class)\n",
    "    classes_100.add(super_class)\n",
    "\n",
    "{k: v for k, v in list(super_classes.items())[:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8b202a-cf6d-4ce6-a66a-b1fd4bf72e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classes_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10dfeaf-14be-4fd7-b003-84973ca08ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class2id_100={c: i for i, c in enumerate(classes_100)}\n",
    "\n",
    "with open(f\"{dest_pkl_folder}class2id.pkl\", \"wb+\") as handle:\n",
    "    dump(class2id_100, handle)\n",
    "\n",
    "id2class_100={i: c for c, i in class2id_100.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2904b78-60e9-4863-bdea-3e3599853807",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in list(class2id_100.items())[:5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8218c5b-a49a-49b0-8c9d-77cb30641cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "instype={\n",
    "    entid: class2id_100[onto_class]\n",
    "    for entid, onto_class in instance_types_100.items()\n",
    "}\n",
    "\n",
    "with open(f\"{dest_pkl_folder}instype.pkl\", \"wb+\") as handle:\n",
    "    dump(instype, handle)\n",
    "\n",
    "{k: v for k, v in list(instype.items())[:5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2961c3-2853-497b-a229-b1bcf9402a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ent2id_100.keys()) - len(instype.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd526e51-a9fc-41c7-b0d1-7363aa372068",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    ent_uri\n",
    "    for ent_uri, ent_id in ent2id_100.items()\n",
    "    if not ent_id in instype.keys()\n",
    "][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dbbac0-7e50-428e-bc93-56c533ca6119",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2id2dom2id = {\n",
    "    rel2id_100[rel]: class2id_100[onto_class]\n",
    "    for rel, onto_class in rel2domain.items()\n",
    "}\n",
    "\n",
    "with open(f\"{dest_pkl_folder}r2id2dom2id.pkl\", \"wb+\") as handle:\n",
    "    dump(r2id2dom2id, handle)\n",
    "\n",
    "r2id2range2id = {\n",
    "    rel2id_100[rel]: class2id_100[onto_class]\n",
    "    for rel, onto_class in rel2range.items()\n",
    "}\n",
    "\n",
    "with open(f\"{dest_pkl_folder}r2id2range2id.pkl\", \"wb+\") as handle:\n",
    "    dump(r2id2range2id, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bacd61-dd07-4a9b-ad13-43ecf628d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in list(r2id2dom2id.items())[:5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5948a9a-748a-4488-ac16-bc21fb9da233",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in list(r2id2range2id.items())[:5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded25d3-706e-4f8f-83b3-6deb6ad45f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "subclassof2id_100 = {\n",
    "    class2id_100[onto_class]: class2id_100[super_class]\n",
    "    for onto_class, super_class in super_classes.items()\n",
    "}\n",
    "\n",
    "with open(f\"{dest_pkl_folder}subclassof2id.pkl\", \"wb+\") as handle:\n",
    "    dump(subclassof2id_100, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fbaaa4-1121-4dfe-ae3d-432d6556fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ancestors(c):\n",
    "    if not c in subclassof2id_100.keys():\n",
    "        return [c]\n",
    "    else:\n",
    "        result = [c]\n",
    "        result.extend(ancestors(subclassof2id_100[c]))\n",
    "        return result\n",
    "\n",
    "def descendants(c):\n",
    "    if not c in subclassof2id_100.values():\n",
    "        return [c]\n",
    "    else:\n",
    "        result = [c]\n",
    "\n",
    "        children = [\n",
    "            child\n",
    "            for child in subclassof2id_100.keys()\n",
    "            if subclassof2id_100[child] == c\n",
    "        ]\n",
    "\n",
    "        for child in children:\n",
    "            result.extend(descendants(child))\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4685700-d30d-4150-a0c1-759ca1207f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "instype_all = {\n",
    "    entid: ancestors(instype[entid]) if entid in instype else []\n",
    "    for entid in ent2id_100.values()\n",
    "}\n",
    "\n",
    "with open(f\"{dest_pkl_folder}instype_all.pkl\", \"wb+\") as handle:\n",
    "    dump(instype_all, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51435982-169f-4244-b6e8-51b8f77b9ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in list(instype_all.items())[:5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5bdd8-9c74-4571-8cde-1da4b2789651",
   "metadata": {},
   "outputs": [],
   "source": [
    "{id2ent_100[k]: [id2class_100[c] for c in v] for k, v in list(instype_all.items())[:5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b0d1b5-28c5-4cb6-9145-355e83467cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([id2ent_100[k] for k, v in list(instype_all.items()) if len(v) == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b38cda9-da89-40fd-a278-f64036a29ff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class2id2ent2id={\n",
    "    class_id: [\n",
    "        ent_id\n",
    "        for ent_id in instype_all.keys()\n",
    "        if class_id in instype_all[ent_id]\n",
    "    ]\n",
    "    for class_id in id2class_100.keys()\n",
    "}\n",
    "\n",
    "with open(f\"{dest_pkl_folder}class2id2ent2id.pkl\", \"wb+\") as handle:\n",
    "    dump(class2id2ent2id, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45fe0c1-8f2b-4afa-93a8-8525280127c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v[:5] for k, v in list(class2id2ent2id.items())[:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9adfc25-3df4-49f9-967d-bd3141edf711",
   "metadata": {},
   "outputs": [],
   "source": [
    "{id2class_100[k]: [id2ent_100[c] for c in v[:5]] for k, v in list(class2id2ent2id.items())[:5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc5393-8269-4296-9855-8b3bdb6a71b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "[id2class_100[k] for k, v in list(class2id2ent2id.items()) if len(v) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0793932-7848-4893-9df0-c19b559b3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_inference = list(set([\n",
    "    (s, onto_class)\n",
    "    \n",
    "    for s in tqdm(observed_tails_original_kg.keys(), total=len(observed_tails_original_kg.keys()))\n",
    "    if not id2ent_100[s] in unknown_entities_100\n",
    "    \n",
    "    for p in observed_tails_original_kg[s].keys()\n",
    "    if p in r2id2dom2id.keys()\n",
    "    \n",
    "    for onto_class in ancestors(r2id2dom2id[p])\n",
    "    if not onto_class in instype_all[s]\n",
    "]))\n",
    "\n",
    "range_inference = list(set([\n",
    "    (o, onto_class)\n",
    "    \n",
    "    for o in tqdm(observed_heads_original_kg.keys(), total=len(observed_heads_original_kg.keys()))\n",
    "    if not id2ent_100[o] in unknown_entities_100\n",
    "    \n",
    "    for p in observed_heads_original_kg[o].keys()\n",
    "    if p in r2id2range2id.keys()\n",
    "    \n",
    "    for onto_class in ancestors(r2id2range2id[p])\n",
    "    if not onto_class in instype_all[o]\n",
    "    and not (o, onto_class) in domain_inference\n",
    "]))\n",
    "\n",
    "signature_inference = domain_inference + range_inference\n",
    "inferred_types = {}\n",
    "\n",
    "for entity, onto_class in tqdm(signature_inference, total=len(signature_inference)):\n",
    "    if entity in inferred_types:\n",
    "        inferred_types[entity].append(onto_class)\n",
    "    else:\n",
    "        inferred_types[entity] = [onto_class]\n",
    "\n",
    "print(\"Entity types inferred by domained predicates\", len(domain_inference))\n",
    "print(\"Entity types inferred by ranged predicates\", len(range_inference))\n",
    "print(\"Entity affected by type inference\", len(inferred_types.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31e537-7173-4cef-8928-4031bb9e8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inferred types for England:\\n\")\n",
    "\n",
    "print(\"\\n\".join([\n",
    "    id2class_100[t]\n",
    "    for t in inferred_types[ent2id_100['<http://dbpedia.org/resource/England>']]\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e203b-f7d9-4abc-bdb1-0bc19bf971f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity='<http://dbpedia.org/resource/England>'\n",
    "onto_class='<http://dbpedia.org/ontology/Genre>'\n",
    "\n",
    "domain_errors = [\n",
    "    (id2rel_100[p], id2class_100[r2id2dom2id[p]]) for p in (\n",
    "        observed_tails_original_kg[ent2id_100[entity]].keys()\n",
    "        if ent2id_100[entity] in observed_tails_original_kg.keys()\n",
    "        else []\n",
    "    )\n",
    "    if p in r2id2dom2id.keys()\n",
    "    and r2id2dom2id[p] in descendants(class2id_100[onto_class])\n",
    "]\n",
    "\n",
    "range_errors = [\n",
    "    (id2rel_100[p], id2class_100[r2id2range2id[p]]) for p in (\n",
    "        observed_heads_original_kg[ent2id_100[entity]].keys()\n",
    "        if ent2id_100[entity] in observed_heads_original_kg.keys()\n",
    "        else []\n",
    "    )\n",
    "    if p in r2id2range2id.keys() and r2id2range2id[p] in descendants(class2id_100[onto_class])\n",
    "]\n",
    "\n",
    "print(f\"Triples responsible for entity '{entity.split('/')[-1][:-1]}' to be entailed of type '{onto_class.split('/')[-1][:-1]}'\\n\")\n",
    "\n",
    "print(\"\\n\".join([\n",
    "    f\"{entity} {rel} {id2ent_100[tail]}\"\n",
    "    for rel, _ in domain_errors\n",
    "    for tail in observed_tails_original_kg[ent2id_100[entity]][rel2id_100[rel]]\n",
    "] + [\n",
    "    f\"{id2ent_100[head]} {rel} {entity} .\"\n",
    "    for rel, _ in range_errors\n",
    "    for head in observed_heads_original_kg[ent2id_100[entity]][rel2id_100[rel]]\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b821103b-43da-4e31-abaa-91f7414086f4",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17aaee0-d262-4a0c-bd56-412796ff234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3481f194-0e75-42d1-961f-f0efb0b729f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2id_100 = None\n",
    "\n",
    "with open(f\"{dest_pkl_folder}rel2id.pkl\", \"rb\") as handle:\n",
    "    rel2id_100 = load(handle)\n",
    "\n",
    "id2rel_100 = {v: k for k, v in rel2id_100.items()}\n",
    "\n",
    "wikidata_dbpedia = None\n",
    "\n",
    "with open(f\"{dest_pkl_folder}wikidata_dbpedia.pkl\", \"rb\") as handle:\n",
    "    wikidata_dbpedia = load(handle)\n",
    "\n",
    "r2id2dom2id = None\n",
    "\n",
    "with open(f\"{dest_pkl_folder}r2id2dom2id.pkl\", \"rb\") as handle:\n",
    "    r2id2dom2id = load(handle)\n",
    "\n",
    "r2id2range2id = None\n",
    "\n",
    "with open(f\"{dest_pkl_folder}r2id2range2id.pkl\", \"rb\") as handle:\n",
    "    r2id2range2id = load(handle)\n",
    "\n",
    "observed_heads_original_kg = None\n",
    "\n",
    "with open(f\"{dest_pkl_folder}observed_heads_original_kg.pkl\", \"rb\") as handle:\n",
    "    observed_heads_original_kg = load(handle)\n",
    "\n",
    "class2id2ent2id = None\n",
    "\n",
    "with open(f\"{dest_pkl_folder}class2id2ent2id.pkl\", \"rb\") as handle:\n",
    "    class2id2ent2id = load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8053f164-ef26-4cad-b621-10a35b274b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "total=len(rel2id_100.values())\n",
    "\n",
    "signed=[\n",
    "    p\n",
    "    for p in rel2id_100.values()\n",
    "    if p in r2id2dom2id.keys() \n",
    "    and p in r2id2range2id.keys()\n",
    "]\n",
    "\n",
    "domain_no_range = [\n",
    "    p\n",
    "    for p in rel2id_100.values()\n",
    "    if p in r2id2dom2id.keys() \n",
    "    and not p in r2id2range2id.keys()\n",
    "]\n",
    "\n",
    "range_no_domain = [\n",
    "    p\n",
    "    for p in rel2id_100.values()\n",
    "    if not p in r2id2dom2id.keys() \n",
    "    and p in r2id2range2id.keys()\n",
    "]\n",
    "\n",
    "not_signed = [\n",
    "    p\n",
    "    for p in rel2id_100.values()\n",
    "    if not p in r2id2dom2id.keys() \n",
    "    and not p in r2id2range2id.keys()\n",
    "]\n",
    "\n",
    "total, int(10000*len(signed)/total)/100, int(10000*len(domain_no_range)/total)/100, int(10000*len(range_no_domain)/total/100), int(10000*len(not_signed)/total)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98272bb9-a6df-4f3f-8070-42860d0a5e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_count={id: 0 for id in rel2id_100.values()}\n",
    "\n",
    "for _, ps in observed_heads_original_kg.items():\n",
    "    for p, _ in ps.items():\n",
    "        predicate_count[p]+=1\n",
    "\n",
    "total_count=sum(predicate_count.values())\n",
    "\n",
    "signed_count=sum([\n",
    "    predicate_count[p]\n",
    "    for p in signed\n",
    "])\n",
    "\n",
    "domain_no_range_count = sum([\n",
    "    predicate_count[p]\n",
    "    for p in domain_no_range\n",
    "])\n",
    "\n",
    "range_no_domain_count = sum([\n",
    "    predicate_count[p]\n",
    "    for p in range_no_domain\n",
    "])\n",
    "\n",
    "not_signed_count = sum([\n",
    "    predicate_count[p]\n",
    "    for p in not_signed\n",
    "])\n",
    "\n",
    "def percent(number, total, precision=4):\n",
    "    return int((10**precision)*number/total)/(10**(precision - 2))\n",
    "\n",
    "print(\"In number of triples:\\n\")\n",
    "print(total_count, signed_count, domain_no_range_count, range_no_domain_count, not_signed_count)\n",
    "\n",
    "print(\"\\nIn percentage of triples:\\n\")\n",
    "print(total_count, percent(signed_count, total_count), percent(domain_no_range_count, total_count), percent(range_no_domain_count, total_count), percent(not_signed_count, total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1bd5fc-23f7-4d09-b0f3-cee2031314a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_domains_instances = {i: len(class2id2ent2id[r2id2dom2id[i]]) for i in r2id2dom2id.keys()}\n",
    "\n",
    "not_instanciated_domains = [k for k in predicate_domains_instances.keys() if predicate_domains_instances[k] == 0]\n",
    "\n",
    "not_instanciated_domains_count = sum([predicate_count[p] for p in not_instanciated_domains])\n",
    "not_instanciated_domains_for_signed_count = sum([predicate_count[p] for p in not_instanciated_domains if p in signed])\n",
    "not_instanciated_domains_for_domained_count = sum([predicate_count[p] for p in not_instanciated_domains if p in domain_no_range])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predicate_ranges_instances = {i: len(class2id2ent2id[r2id2range2id[i]]) for i in r2id2range2id.keys()}\n",
    "\n",
    "not_instanciated_ranges = [k for k in predicate_ranges_instances.keys() if predicate_ranges_instances[k] == 0]\n",
    "\n",
    "not_instanciated_ranges_count = sum([predicate_count[p] for p in not_instanciated_ranges])\n",
    "not_instanciated_ranges_for_signed_count = sum([predicate_count[p] for p in not_instanciated_ranges if p in signed])\n",
    "not_instanciated_ranges_for_ranged_count = sum([predicate_count[p] for p in not_instanciated_ranges if p in range_no_domain])\n",
    "\n",
    "print(\n",
    "    \"These domained predicates have no instances:\\n\",\n",
    "    \"\\t\" + \"\\n\\t\".join([id2rel_100[p] for p in not_instanciated_domains])\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"\\n These ranged predicates have no instances:\\n\",\n",
    "    \"\\t\" + \"\\n\\t\".join([id2rel_100[p] for p in not_instanciated_ranges])\n",
    ")\n",
    "\n",
    "print(\"\\nOverlap between these sets?\", len([x for x in not_instanciated_ranges if x in not_instanciated_domains]) > 0)\n",
    "\n",
    "not_instanciated_signatures_for_signed_count = (not_instanciated_domains_for_signed_count + not_instanciated_ranges_for_signed_count)\n",
    "\n",
    "print(\n",
    "    \"\\nImpact on signed triples: \",\n",
    "    f\"{not_instanciated_signatures_for_signed_count} ({percent(not_instanciated_signatures_for_signed_count, signed_count)}% of fully signed triples)\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Impact on domained triples: \",\n",
    "    f\"{not_instanciated_domains_for_domained_count} ({percent(not_instanciated_domains_for_domained_count, domain_no_range_count)}% of domained triples)\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Impact on ranged triples: \",\n",
    "    f\"{not_instanciated_ranges_for_ranged_count} ({percent(not_instanciated_ranges_for_ranged_count, range_no_domain_count)}% of ranged triples)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38130af-ee90-44a4-a4ed-fbde5d9edd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_divider=4\n",
    "y_divider=20\n",
    "y_offset=0.65\n",
    "\n",
    "nodes = {\n",
    "    0: {\n",
    "        'label': 'Total',\n",
    "        'x': 0/x_divider,\n",
    "        'y': 0/y_divider+y_offset\n",
    "    },\n",
    "    1: {\n",
    "        'label': 'Full sign', \n",
    "        'x': 1/x_divider,\n",
    "        'y': 2/y_divider+y_offset\n",
    "    },\n",
    "    2: {\n",
    "        'label': 'Incomplete sign',\n",
    "        'x': 1/x_divider,\n",
    "        'y': -5/y_divider+y_offset\n",
    "    },\n",
    "    3: {\n",
    "        'label': 'Half sign',\n",
    "        'x': 2/x_divider,\n",
    "        'y': -4/y_divider+y_offset\n",
    "    },\n",
    "    4: {\n",
    "        'label': 'No sign',\n",
    "        'x': 2/x_divider,\n",
    "        'y': -9/y_divider+y_offset\n",
    "    },\n",
    "    5: {\n",
    "        'label': 'Only domain',\n",
    "        'x': 3/x_divider,\n",
    "        'y': 1/y_divider+y_offset\n",
    "    },\n",
    "    6: {\n",
    "        'label': 'Only range',\n",
    "        'x': 3/x_divider,\n",
    "        'y': -7/y_divider+y_offset\n",
    "    },\n",
    "    7: {\n",
    "        'label': 'No domain instance',\n",
    "        'x': 4/x_divider,\n",
    "        'y': 1/y_divider+y_offset\n",
    "    },\n",
    "    8: {\n",
    "        'label': 'Domain instance',\n",
    "        'x': 4/x_divider,\n",
    "        'y': -4/y_divider+y_offset\n",
    "    },\n",
    "    9: {\n",
    "        'label': 'No range instance',\n",
    "        'x': 4/x_divider,\n",
    "        'y': -8/y_divider+y_offset\n",
    "    },\n",
    "    10: {\n",
    "        'label': 'Range instance',\n",
    "        'x': 2/x_divider,\n",
    "        'y': 7/y_divider+y_offset\n",
    "    },\n",
    "    11: {\n",
    "        'label': 'No sign instance',\n",
    "        'x': 2/x_divider,\n",
    "        'y': 3/y_divider+y_offset #3\n",
    "    },\n",
    "    12: {\n",
    "        'label': 'Sign instance',\n",
    "        'x': 2/x_divider,\n",
    "        'y': 1/y_divider+y_offset\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec62e0-17f9-4305-a3af-2aacfa7b675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [\n",
    "    (\n",
    "        'Total',\n",
    "        'Full sign',\n",
    "        signed_count\n",
    "    ),\n",
    "    (\n",
    "        'Total',\n",
    "        'Incomplete sign',\n",
    "        total_count - signed_count\n",
    "    ),\n",
    "    (\n",
    "        'Full sign',\n",
    "        'No sign instance',\n",
    "        not_instanciated_signatures_for_signed_count\n",
    "    ),\n",
    "    (\n",
    "        'Full sign',\n",
    "        'Sign instance',\n",
    "        signed_count - not_instanciated_signatures_for_signed_count\n",
    "    ),\n",
    "    (\n",
    "        'Incomplete sign',\n",
    "        'No sign',\n",
    "        not_signed_count\n",
    "    ),\n",
    "    (\n",
    "        'Incomplete sign',\n",
    "        'Half sign',\n",
    "        total_count - signed_count - not_signed_count\n",
    "    ),\n",
    "    (\n",
    "        'Half sign',\n",
    "        'Only domain',\n",
    "        domain_no_range_count\n",
    "    ),\n",
    "    (\n",
    "        'Half sign',\n",
    "        'Only range',\n",
    "        range_no_domain_count\n",
    "    ),\n",
    "    (\n",
    "        'Only domain',\n",
    "        'No domain instance',\n",
    "        not_instanciated_domains_for_domained_count\n",
    "    ),\n",
    "    (\n",
    "        'Only domain',\n",
    "        'Domain instance',\n",
    "        domain_no_range_count - not_instanciated_domains_for_domained_count\n",
    "    ),\n",
    "    (\n",
    "        'Only range',\n",
    "        'No range instance',\n",
    "        not_instanciated_ranges_for_ranged_count\n",
    "    ),\n",
    "    (\n",
    "        'Only range',\n",
    "        'Range instance',\n",
    "        range_no_domain_count - not_instanciated_ranges_for_ranged_count\n",
    "    )\n",
    "]\n",
    "\n",
    "links=[\n",
    "    (\n",
    "        [k for k in nodes.keys() if nodes[k][\"label\"] == source][0],\n",
    "        [k for k in nodes.keys() if nodes[k][\"label\"] == target][0],\n",
    "        value        \n",
    "    )\n",
    "    for source, target, value in links \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d231e2a5-615d-4545-ac31-4890a005fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Sankey(\n",
    "            node = dict(\n",
    "                pad = 15,\n",
    "                thickness = 20,\n",
    "                line = dict(color = \"black\", width = 0.5),\n",
    "                label = [nodes[i][\"label\"] for i in nodes.keys()],\n",
    "                x = [nodes[i][\"x\"] for i in nodes.keys()],\n",
    "                y = [nodes[i][\"y\"] for i in nodes.keys()],\n",
    "                color = \"blue\"\n",
    "            ),\n",
    "    link = dict(\n",
    "        source = [source for source, _, _ in links],\n",
    "        target = [target for _, target, _ in links],\n",
    "        value = [value for _, _, value in links]\n",
    "  ))])\n",
    "\n",
    "fig.update_layout(title_text=f\"Domain/range coverage over triples of dataset {destination_folder.split('/')[-2]}\", font_size=10)\n",
    "fig.show()\n",
    "fig.write_image(f\"{destination_folder}fig1.png\", engine='orca', width=1500, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871da26f-b597-412d-956d-226dff33c4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reproduction",
   "language": "python",
   "name": "reproduction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
